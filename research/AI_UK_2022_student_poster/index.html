<!DOCTYPE html>

<html lang="en">
    <head>
        <title>Lele Liu - Research</title>
        <meta charset="utf-8">
        <link rel="stylesheet" href="../../css/bootstrap.min.css">
        <link rel="stylesheet" href="../../css/font-awesome.min.css">
        <link rel="stylesheet" href="../../css/style.css">
    </head>

    <body>
        <div class="nav-bar">
            <div class="nav-wrapper">
                <div style="text-align:left">
                    <h1 class="heading">
                        <img src="../../image/profile.jpg" style="width:1.4em;border-radius:0.7em"></img>
                        <em>Lele Liu</em>
                    </h1>
                    <span class="nav" id="nav_1" style="cursor:pointer">Home</span>
                    <span class="nav" id="nav_2" style="cursor:pointer">Publications</span>
                </div>
            </div>
        </div>
        <div class="mybackground">
            <div class="page">
                <div class="page-wrapper">

                    <h1>AI UK 2022 - Turing PhD, enrichment and visiting student poster session</h1>
                    <p></b>
                    <p>March 22, 2022</p>
                    <hr class="solid">
                    
                    <p><img style="width:100%" src="./L. Liu.jpg" alt="poster"></p>

                    <p>We demonstrate our work on deep learning methodologies applied for automatic music transcription (AMT), a core problem in the field of data science for music. AMT is commonly considered as the music-equivalent of automatic speech recognition, which works on converting music signals into some form of human- or machine-readable music notation, such as western staff notation. Various applications of AMT include music recommendation, interactive music systems, and digital musicology.</p>

                    <p>There are generally two types of methods used in audio-to-score transcription, namely: 1) end-to-end methods that consist of deep learning models that directly convert music signals into musical scores, and 2) step-by-step methods that firstly predict note sequences from music audio, and then predict the rhythm structure based on the predicted notes.</p>

                    <p>In the poster, we present our work on automatic piano transcription by exploring the two above-mentioned methods. We explain the workflow with diagrams and prediction examples. The end-to-end method we developed (paper published in IEEE ICASSP 2021) is based on a joint pitch detection and score transcription model. The model uses a recurrent neural network-based sequence-to-sequence structure with an attention mechanism and can work for transcribing popular piano music. The step-by-step method (our current work) is designed for classical piano music transcription and is based on a convolutional recurrent neural network structure.</p>

                    <p>In the end, we include a short discussion on the limitations of our methods and our next steps.</p>

                    <h2>Related materials</h2>
                    <hr class="dashed">

                    <h3>References</h3>

                    <p>Q. Kong, B. Li, X. Song, Y. Wan, and Y. Wang, “<em><a href="https://arxiv.org/abs/2010.01815">High-resolution Piano Transcription with Pedals by Regressing Onsets and Offsets Times</a></em>," arXiv preprint arXiv:2010.01815, pp. 1–10, 2020.</p>

                    <p>L. Liu, V. Morfi and E. Benetos, "<em><a href="https://archives.ismir.net/ismir2021/latebreaking/000013.pdf">ACPAS: A Dataset of Aligned Classical Piano Audio and Scores for Audio-to-Score Transcription</a></em>," International Society for Music Information Retrieval Conference Late-Breaking Demos Session, Nov 2021.</p> 

                    <p>L. Liu and E. Benetos, "<em><a href="https://link.springer.com/chapter/10.1007/978-3-030-72116-9_24">From Audio to Music Notation</a></em>," in Handbook of Artificial Intelligence for Music, E. Miranda (ed.), pp. 693-714, Springer, 2021 (ISBN: 978-3-030-72115-2).</p>

                    <p>L. Liu, V. Morfi and E. Benetos, "<em><a href="https://ieeexplore.ieee.org/document/9413601">Joint Multi-pitch Detection and Score Transcription for Polyphonic Piano Music</a></em>," IEEE International Conference on Acoustics, Speech and Signal Processing, Canada, Jun 2021.</p>

                    <h3>Datasets</h3>

                    <p>MuseSyn dataset: <a href="https://zenodo.org/record/4527460#.YjmkTzfP3VY">[link]</a></p>

                    <p>ACPAS dataset: <a href="https://cheriell.github.io/research/ACPAS_dataset">[link]</a></p>


                </div>
            </div>
            <div class="footer">
                <div class="footer-wrapper">
                    <span style="padding-left:1em">Copyright <i class="fa fa-copyright" aria-hidden="true"></i> Lele Liu</span>
                </div>
            </div>
        </div>
    </body>
</html>


<script src="../../js/ajax.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script src="../../js/jquery-1.11.2.js"></script>
<script src='../../js/navigate.js'></script>